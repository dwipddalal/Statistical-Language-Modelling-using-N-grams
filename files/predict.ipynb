{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering, TrainingArguments, Trainer,AutoConfig\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TrainingArguments\n",
    "from transformers import HfArgumentParser\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import DistilBertModel\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedModel,PretrainedConfig\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/vp.shivasan/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "my_dataset = squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3a84653c23428e9df7f674692efdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be9b99878a64abeac1c7ed3e15598c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"context\"] = examples[\"context\"]\n",
    "    inputs[\"answer\"] = answers\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERTQA(PreTrainedModel):\n",
    "    def __init__(self,config: PretrainedConfig):\n",
    "        # super(DistillBERTQA, selfconfig).__init__()\n",
    "        super().__init__(config)\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.qa_outputs = torch.nn.Linear(768, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,start_positions=None,end_positions=None,return_dict=None):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = distilbert_output[0]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + distilbert_output[1:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistillBERTQA(config)\n",
    "\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = \"/home/vp.shivasan/interiit/task2/training_dir/squad_test\",\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 16,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "              model = model, \n",
    "              args = test_args, \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metric calculation taken from https://rajpurkar.github.io/SQuAD-explorer/\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0,0,0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return precision,recall,f1\n",
    "\n",
    "preds,labels,metrics = trainer.predict(tokenized_data['test'])\n",
    "start_idxs = np.argmax(preds[0],axis=1)\n",
    "end_idxs = np.argmax(preds[1],axis=1)\n",
    "contexts = tokenized_data['test']['context']\n",
    "answers = tokenized_data['test']['answer']\n",
    "assert len(contexts) == len(start_idxs) == len(answers) \n",
    "F1s = []\n",
    "Precs = []\n",
    "Recs = []\n",
    "for i,(sidx,eidx) in enumerate(zip(start_idxs,end_idxs)):\n",
    "    context_para = contexts[i]\n",
    "    pred_answer = context_para[sidx:eidx+1]\n",
    "    gold_answer = answers[i]['text'][0]\n",
    "    p,r,f = compute_f1(gold_answer,pred_answer)\n",
    "    Precs.append(p)\n",
    "    Recs.append(r)\n",
    "    F1s.append(f)\n",
    "\n",
    "print(\"Average Recall score: \",np.mean(Recs))\n",
    "print(\"Average Precision score: \",np.mean(Precs))\n",
    "print(\"Average F1 score: \",np.mean(F1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('/home/vp.shivasan/interiit/data/Task2dataSet_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistillBERTQA(PreTrainedModel):\n",
    "    def __init__(self,config: PretrainedConfig):\n",
    "        # super(DistillBERTQA, config).__init__()\n",
    "        super().__init__(config)\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.qa_outputs = torch.nn.Linear(768, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,start_positions=None,end_positions=None,return_dict=None):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = distilbert_output[0]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + distilbert_output[1:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/vp.shivasan/interiit/task2/training_dir/task2_50epochs_2e-5_ES/checkpoint-5076/\")\n",
    "config = AutoConfig.from_pretrained(\"/home/vp.shivasan/interiit/task2/training_dir/task2_50epochs_2e-5_ES/checkpoint-5076/\")\n",
    "model = DistillBERTQA(config)\n",
    "state_dict = torch.load('/home/vp.shivasan/interiit/task2/training_dir/task2_50epochs_2e-5_ES/checkpoint-5076/pytorch_model.bin')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076 were not used when initializing BertModel: ['distilbert.encoder.layer.10.output.dense.weight', 'distilbert.encoder.layer.0.attention.self.value.bias', 'distilbert.encoder.layer.8.output.LayerNorm.bias', 'distilbert.encoder.layer.2.output.LayerNorm.bias', 'distilbert.embeddings.position_ids', 'distilbert.encoder.layer.0.output.dense.weight', 'distilbert.encoder.layer.11.attention.self.value.weight', 'distilbert.encoder.layer.1.output.dense.weight', 'distilbert.encoder.layer.3.attention.self.query.bias', 'distilbert.encoder.layer.11.attention.output.dense.bias', 'distilbert.encoder.layer.4.output.dense.bias', 'distilbert.pooler.dense.weight', 'distilbert.encoder.layer.8.intermediate.dense.bias', 'distilbert.encoder.layer.6.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.9.attention.self.value.bias', 'distilbert.encoder.layer.4.attention.self.query.bias', 'distilbert.encoder.layer.9.output.LayerNorm.bias', 'distilbert.encoder.layer.5.attention.self.key.bias', 'distilbert.encoder.layer.1.intermediate.dense.bias', 'distilbert.encoder.layer.4.attention.output.dense.weight', 'distilbert.encoder.layer.6.output.dense.weight', 'distilbert.encoder.layer.9.output.dense.bias', 'distilbert.encoder.layer.1.output.LayerNorm.weight', 'distilbert.encoder.layer.5.attention.self.value.weight', 'distilbert.encoder.layer.7.attention.self.key.bias', 'distilbert.encoder.layer.8.attention.output.dense.bias', 'distilbert.encoder.layer.10.attention.output.dense.bias', 'distilbert.encoder.layer.9.attention.output.dense.weight', 'distilbert.encoder.layer.8.attention.self.query.weight', 'distilbert.encoder.layer.0.intermediate.dense.weight', 'distilbert.encoder.layer.3.attention.output.dense.weight', 'distilbert.encoder.layer.2.attention.output.dense.weight', 'distilbert.encoder.layer.4.output.dense.weight', 'distilbert.encoder.layer.5.attention.self.query.weight', 'distilbert.encoder.layer.4.intermediate.dense.bias', 'distilbert.encoder.layer.6.attention.self.query.weight', 'distilbert.encoder.layer.5.attention.self.key.weight', 'distilbert.encoder.layer.7.output.dense.bias', 'distilbert.encoder.layer.10.attention.self.key.bias', 'distilbert.encoder.layer.1.attention.output.dense.bias', 'distilbert.encoder.layer.11.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.2.attention.self.value.bias', 'distilbert.encoder.layer.7.intermediate.dense.weight', 'distilbert.encoder.layer.0.output.LayerNorm.bias', 'distilbert.pooler.dense.bias', 'distilbert.encoder.layer.3.output.dense.bias', 'distilbert.encoder.layer.1.attention.self.query.bias', 'distilbert.encoder.layer.11.intermediate.dense.weight', 'distilbert.encoder.layer.2.attention.self.query.bias', 'distilbert.encoder.layer.4.attention.self.key.bias', 'distilbert.encoder.layer.9.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.4.output.LayerNorm.weight', 'distilbert.encoder.layer.7.attention.self.value.bias', 'distilbert.encoder.layer.3.attention.self.query.weight', 'qa_outputs.bias', 'distilbert.encoder.layer.3.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.9.attention.self.key.weight', 'distilbert.encoder.layer.10.output.LayerNorm.weight', 'distilbert.encoder.layer.0.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.6.output.dense.bias', 'distilbert.encoder.layer.2.intermediate.dense.weight', 'distilbert.encoder.layer.10.attention.self.query.weight', 'distilbert.encoder.layer.7.output.LayerNorm.bias', 'distilbert.encoder.layer.11.attention.self.value.bias', 'distilbert.encoder.layer.9.attention.self.key.bias', 'distilbert.encoder.layer.6.attention.output.LayerNorm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.encoder.layer.0.attention.self.key.bias', 'distilbert.encoder.layer.5.attention.self.value.bias', 'distilbert.encoder.layer.9.attention.self.query.weight', 'distilbert.encoder.layer.0.attention.self.key.weight', 'qa_outputs.weight', 'distilbert.encoder.layer.2.attention.output.dense.bias', 'distilbert.encoder.layer.4.attention.self.value.weight', 'distilbert.encoder.layer.4.intermediate.dense.weight', 'distilbert.encoder.layer.5.attention.self.query.bias', 'distilbert.encoder.layer.5.output.dense.bias', 'distilbert.encoder.layer.3.output.LayerNorm.weight', 'distilbert.encoder.layer.6.attention.output.dense.bias', 'distilbert.encoder.layer.10.attention.self.value.bias', 'distilbert.encoder.layer.7.attention.output.dense.bias', 'distilbert.encoder.layer.3.attention.self.value.weight', 'distilbert.encoder.layer.3.attention.output.dense.bias', 'distilbert.encoder.layer.5.attention.output.dense.bias', 'distilbert.encoder.layer.7.output.LayerNorm.weight', 'distilbert.encoder.layer.1.attention.self.key.weight', 'distilbert.encoder.layer.7.attention.self.value.weight', 'distilbert.encoder.layer.7.output.dense.weight', 'distilbert.encoder.layer.3.output.dense.weight', 'distilbert.encoder.layer.10.attention.self.key.weight', 'distilbert.encoder.layer.8.intermediate.dense.weight', 'distilbert.encoder.layer.1.attention.self.value.weight', 'distilbert.encoder.layer.3.attention.self.key.weight', 'distilbert.encoder.layer.8.output.LayerNorm.weight', 'distilbert.encoder.layer.5.intermediate.dense.weight', 'distilbert.encoder.layer.8.attention.self.key.bias', 'distilbert.encoder.layer.11.attention.output.dense.weight', 'distilbert.embeddings.token_type_embeddings.weight', 'distilbert.encoder.layer.5.output.LayerNorm.weight', 'distilbert.encoder.layer.8.output.dense.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.encoder.layer.10.attention.self.value.weight', 'distilbert.encoder.layer.6.attention.self.value.weight', 'distilbert.encoder.layer.10.intermediate.dense.weight', 'distilbert.encoder.layer.8.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.10.output.dense.bias', 'distilbert.encoder.layer.11.attention.self.query.bias', 'distilbert.encoder.layer.2.output.dense.weight', 'distilbert.encoder.layer.2.output.LayerNorm.weight', 'distilbert.encoder.layer.7.attention.self.query.bias', 'distilbert.encoder.layer.0.intermediate.dense.bias', 'distilbert.encoder.layer.6.intermediate.dense.bias', 'distilbert.encoder.layer.0.output.dense.bias', 'distilbert.encoder.layer.7.intermediate.dense.bias', 'distilbert.encoder.layer.4.attention.output.dense.bias', 'distilbert.encoder.layer.8.attention.self.key.weight', 'distilbert.encoder.layer.11.intermediate.dense.bias', 'distilbert.encoder.layer.10.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.1.attention.output.dense.weight', 'distilbert.encoder.layer.6.attention.self.key.weight', 'distilbert.encoder.layer.0.attention.self.query.bias', 'distilbert.encoder.layer.3.intermediate.dense.weight', 'distilbert.encoder.layer.8.output.dense.bias', 'distilbert.encoder.layer.11.attention.self.query.weight', 'distilbert.encoder.layer.11.output.dense.bias', 'distilbert.encoder.layer.9.attention.output.dense.bias', 'distilbert.encoder.layer.0.attention.output.dense.weight', 'distilbert.encoder.layer.5.attention.output.dense.weight', 'distilbert.encoder.layer.2.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.1.attention.self.value.bias', 'distilbert.encoder.layer.6.attention.self.value.bias', 'distilbert.encoder.layer.11.attention.self.key.weight', 'distilbert.encoder.layer.2.attention.self.key.weight', 'distilbert.encoder.layer.3.output.LayerNorm.bias', 'distilbert.encoder.layer.6.attention.output.dense.weight', 'distilbert.encoder.layer.9.attention.self.value.weight', 'distilbert.encoder.layer.10.attention.output.dense.weight', 'distilbert.encoder.layer.11.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.1.output.LayerNorm.bias', 'distilbert.encoder.layer.7.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.8.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.1.output.dense.bias', 'distilbert.encoder.layer.11.attention.self.key.bias', 'distilbert.encoder.layer.2.attention.self.value.weight', 'distilbert.encoder.layer.6.output.LayerNorm.weight', 'distilbert.encoder.layer.11.output.LayerNorm.weight', 'distilbert.encoder.layer.3.intermediate.dense.bias', 'distilbert.encoder.layer.1.attention.self.key.bias', 'distilbert.encoder.layer.1.attention.self.query.weight', 'distilbert.encoder.layer.5.output.dense.weight', 'distilbert.encoder.layer.6.attention.self.key.bias', 'distilbert.encoder.layer.0.attention.self.query.weight', 'distilbert.encoder.layer.7.attention.self.query.weight', 'distilbert.encoder.layer.2.intermediate.dense.bias', 'distilbert.encoder.layer.4.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.4.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.9.intermediate.dense.weight', 'distilbert.encoder.layer.1.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.4.attention.self.value.bias', 'distilbert.encoder.layer.10.attention.self.query.bias', 'distilbert.encoder.layer.7.attention.self.key.weight', 'distilbert.encoder.layer.7.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.2.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.10.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.8.attention.self.value.bias', 'distilbert.encoder.layer.3.attention.self.key.bias', 'distilbert.encoder.layer.8.attention.self.query.bias', 'distilbert.encoder.layer.11.output.LayerNorm.bias', 'distilbert.encoder.layer.5.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.4.output.LayerNorm.bias', 'distilbert.encoder.layer.3.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.1.intermediate.dense.weight', 'distilbert.encoder.layer.2.attention.self.query.weight', 'distilbert.encoder.layer.0.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.4.attention.self.query.weight', 'distilbert.encoder.layer.7.attention.output.dense.weight', 'distilbert.encoder.layer.11.output.dense.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.encoder.layer.10.output.LayerNorm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.encoder.layer.8.attention.self.value.weight', 'distilbert.encoder.layer.2.output.dense.bias', 'distilbert.encoder.layer.5.intermediate.dense.bias', 'distilbert.encoder.layer.9.output.LayerNorm.weight', 'distilbert.encoder.layer.9.attention.self.query.bias', 'distilbert.encoder.layer.10.intermediate.dense.bias', 'distilbert.encoder.layer.0.attention.output.dense.bias', 'distilbert.encoder.layer.5.attention.output.LayerNorm.weight', 'distilbert.encoder.layer.3.attention.self.value.bias', 'distilbert.encoder.layer.4.attention.self.key.weight', 'distilbert.encoder.layer.9.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.5.output.LayerNorm.bias', 'distilbert.encoder.layer.6.attention.self.query.bias', 'distilbert.encoder.layer.9.intermediate.dense.bias', 'distilbert.encoder.layer.8.attention.output.dense.weight', 'distilbert.encoder.layer.0.output.LayerNorm.weight', 'distilbert.encoder.layer.1.attention.output.LayerNorm.bias', 'distilbert.encoder.layer.2.attention.self.key.bias', 'distilbert.encoder.layer.9.output.dense.weight', 'distilbert.encoder.layer.0.attention.self.value.weight', 'distilbert.encoder.layer.6.output.LayerNorm.bias', 'distilbert.encoder.layer.6.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076 and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076')\n",
    "model = AutoModel.from_pretrained('/home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076',local_files_only = True,return_dict = True)\n",
    "config = AutoConfig.from_pretrained('/home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076')\n",
    "# model = AutoModel.from_config('/home/vp.shivasan/interiit/task2/training_dir/task2_MiniLM_50epochs_2e-5_FINAL_ES/checkpoint-5076/config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertModel' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'YosoForQuestionAnswering'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipelined_QA = pipeline(task = \"question-answering\",model= model, config = config,tokenizer = tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Tennis is a racket sport that is played either individually against a single opponent (singles) or between two teams of two players each (doubles). Each player uses a tennis racket that is strung with cord to strike a hollow rubber ball covered with felt over or around a net and into the opponent's court. The object of the game is to manoeuvre the ball in such a way that the opponent is not able to play a valid return. The player who is unable to return the ball validly will not gain a point, while the opposite player will.\"\n",
    "question = \"What is the ball made of?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054, 2003, 1996, 3608, 2081, 1997, 1029,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelined_QA(question=question, context=para)['answer']\n",
    "tokenizer(question,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.3559,  0.6228,  0.0090,  ..., -0.7749,  0.9461,  0.0315],\n",
       "         [ 0.6609,  1.2548,  0.4564,  ..., -1.0720,  0.4900,  0.0694],\n",
       "         [ 0.5250,  0.8569,  0.3972,  ..., -0.5089,  0.6350,  0.1445],\n",
       "         ...,\n",
       "         [ 0.1561,  0.3075,  0.5793,  ..., -0.9216,  0.4530,  0.3805],\n",
       "         [ 0.8335,  0.6102,  0.7760,  ..., -0.7382,  0.5135,  0.1184],\n",
       "         [ 1.2206,  0.8561,  0.8853,  ..., -0.9149,  0.8376, -0.0383]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    outtt = model(**inputs,return_dict=True)\n",
    "\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)['start_logits'],model(**inputs)['end_logits']\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    \n",
    "outtt#['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ricoh'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(test_df['answers'][0])['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score:  0.00019948134849391582\n"
     ]
    }
   ],
   "source": [
    "F1s = []\n",
    "for i in range(len(test_df)):\n",
    "    context = test_df['context'][i]\n",
    "    question = test_df['question'][i]\n",
    "    gold_answer = ast.literal_eval(test_df['answers'][0])['text'][0]\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\",truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,return_dict =True)\n",
    "    # print(outputs)\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    # print(answer_start_index,answer_end_index)\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    pred_answer = tokenizer.decode(predict_answer_tokens)\n",
    "    f = compute_f1(gold_answer,pred_answer)\n",
    "    F1s.append(f)\n",
    "    # break\n",
    "print(\"Average F1 score: \",np.mean(F1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChartIE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7cc60da36afb68029bad98d2c120ef528bc696932aa2b6711f6174fe2cafae2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
